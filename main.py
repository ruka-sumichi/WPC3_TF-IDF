import pandas as pd
from preprocess import normalize_all_names
from vectorizer import get_vectors
from matcher import find_top3_matches
from output_writer import write_output

import pandas as pd
from preprocess import normalize_all_names
from vectorizer import get_vectors
from matcher import find_top3_matches
from output_writer import write_output
from tqdm import tqdm

def main():
    # ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆA = æ¯”è¼ƒå¯¾è±¡, B = ã‚¯ã‚¨ãƒªæ–½è¨­ï¼‰
    base_path = r"C:\Sumichika\WPC__AImenu\data\fuzi.csv"    # æ–½è¨­Aï¼ˆãƒ™ãƒ¼ã‚¹ï¼‰
    query_path = r"C:\Sumichika\WPC__AImenu\data\nihon.csv"  # æ–½è¨­Bï¼ˆã‚¯ã‚¨ãƒªï¼‰

    # ğŸ” ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_base = pd.read_csv(base_path, encoding="cp932")
    df_query = pd.read_csv(query_path, encoding="cp932")

    # ğŸ” ã‚¯ã‚¨ãƒªå´ï¼šæ–™ç†ã‚³ãƒ¼ãƒ‰ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼ˆï¼æ¨ªæŒã¡åŒ–ï¼‰
    df_query_unique = df_query.drop_duplicates("nm051_code").copy()

    # ğŸ” æ­£è¦åŒ–ï¼ˆæ–™ç†ã‚³ãƒ¼ãƒ‰å˜ä½ã§ï¼‰
    df_query_unique["normalized_name"] = normalize_all_names(df_query_unique["nm051_dsp_name"].tolist())

    # ğŸ” æ­£è¦åŒ–ï¼ˆä»£è¡¨æ–™ç†åã«å¯¾ã—ã¦ã®ã¿ï¼‰
    query_names_raw = df_query_unique["nm051_dsp_name"].tolist()
    query_names = df_query_unique["normalized_name"].tolist()
    query_codes = df_query_unique["nm051_code"].tolist()

    # âœ… ãƒ‡ãƒãƒƒã‚°ï¼šã‚¯ã‚¨ãƒªå´
    print("[DEBUG] ã‚¯ã‚¨ãƒª: å…¨è¡Œä»¶æ•°ï¼ˆrawï¼‰ =", len(df_query))
    print("[DEBUG] ã‚¯ã‚¨ãƒª: nm051_code ãƒ¦ãƒ‹ãƒ¼ã‚¯ä»¶æ•° =", len(df_query["nm051_code"].unique()))
    print("[DEBUG] ã‚¯ã‚¨ãƒª: æ­£è¦åŒ–å¾Œãƒ¦ãƒ‹ãƒ¼ã‚¯ä»¶æ•° =", len(set(query_names)))


    df_base_grouped = df_base.groupby("nm051_code").agg({
        "nm051_dsp_name": "first",
        "nm013_name": lambda x: "ã€".join(sorted(set(x.dropna())))
    }).reset_index()

    # ğŸ” ãƒ™ãƒ¼ã‚¹å´ï¼ˆæ–½è¨­Aï¼‰ã¯å…¨è¡Œå¯¾è±¡
    base_names_raw = df_base_grouped["nm051_dsp_name"].tolist()
    base_names = normalize_all_names(base_names_raw)
    base_codes = df_base_grouped["nm051_code"].tolist()

    base_ingredients_map = df_base_grouped.set_index("nm051_code")["nm013_name"].to_dict()

    print("[INFO] æ¨ªæŒã¡å¾Œã®ã‚¯ã‚¨ãƒªä»¶æ•°ï¼ˆBæ–½è¨­ï¼‰:", len(query_names))
    print("[INFO] æ¨ªæŒã¡å¾Œã®ãƒ™ãƒ¼ã‚¹ä»¶æ•°ï¼ˆAæ–½è¨­ï¼‰:", len(base_names))

    # ğŸ” ææ–™æƒ…å ±ï¼ˆå¿…è¦ãªã‚‰è¡¨ç¤ºç”¨ã«ï¼‰
    query_ingredients_map = df_query.groupby("nm051_code")["nm013_name"] \
        .apply(lambda x: "ã€".join(sorted(set(x.dropna())))).to_dict()
    base_ingredients_map = df_base.groupby("nm051_code")["nm013_name"] \
        .apply(lambda x: "ã€".join(sorted(set(x.dropna())))).to_dict()

    # âœ… ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆãƒ™ãƒ¼ã‚¹ã¯1å›ã§æ¸ˆã‚€ï¼‰
    _, base_embeddings = get_vectors(["ãƒ€ãƒŸãƒ¼"], base_names)

    # ğŸ” ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆä¾‹ï¼š500ä»¶ãšã¤ï¼‰
    chunk_size = 500
    all_results = []

    for i in tqdm(range(0, len(query_names), chunk_size), desc="ã‚¯ã‚¨ãƒªå‡¦ç†ä¸­", unit="chunk"):
        q_names_chunk = query_names[i:i+chunk_size]
        q_names_raw_chunk = query_names_raw[i:i+chunk_size]
        q_codes_chunk = query_codes[i:i+chunk_size]

        q_embeddings, _ = get_vectors(q_names_chunk, base_names)

        results_chunk = find_top3_matches(
            query_names=q_names_raw_chunk,
            query_codes=q_codes_chunk,
            query_embeddings=q_embeddings,
            base_names=base_names_raw,
            base_codes=base_codes,
            base_embeddings=base_embeddings,
            top_k_final=3,
            query_ingredients_map=query_ingredients_map,
            base_ingredients_map=base_ingredients_map
        )

        all_results.extend(results_chunk)

    # âœ… å‡ºåŠ›
    output_path = "output/matched_top3.csv"
    write_output(all_results, output_path)

if __name__ == "__main__":
    main()


""" def main():
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
    facility_a_path = r"C:\Sumichika\WPC3_TF-IDF\data\A_comparison.csv"
    base_path = r"C:\Sumichika\WPC3_TF-IDF\data\B_base.csv"

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_a = pd.read_csv(facility_a_path)
    df_base = pd.read_csv(base_path)

    df_query_unique = df_a.drop_duplicates("food_code")

    query_names_raw = df_query_unique["food_name"].tolist()                   # åŠ å·¥ã—ã¦ã„ãªã„é£Ÿå“å
    query_codes = df_query_unique["food_code"].tolist()    # æ•´ãˆãŸé£Ÿå“å
    query_names = normalize_all_names(query_names_raw)                     # ã‚³ãƒ¼ãƒ‰

    base_names = normalize_all_names(df_base["food_name"].tolist())
    base_codes = df_base["food_code"].tolist()
    base_names_raw = df_base["food_name"].tolist()

    # TF-IDF ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ï¼ˆquery ã¨ base ã‚’åŒæ™‚ã«æ¸¡ã™ï¼‰
    query_embeddings, base_embeddings = get_vectors(query_names, base_names)

    # 1ä»¶ãšã¤é¡ä¼¼ãƒãƒƒãƒå‡¦ç†
    results = find_top3_matches(
        query_names=query_names_raw, 
        query_codes=query_codes,                   
        query_embeddings=query_embeddings,
        base_names=base_names_raw,
        base_codes=base_codes,
        base_embeddings=base_embeddings,
        top_k_final=3
    )

    # å‡ºåŠ›
    output_path = "output/matched_top3.csv"
    write_output(results, output_path)

if __name__ == "__main__":
    main()
 """